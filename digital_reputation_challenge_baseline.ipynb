{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T08:33:35.319634Z",
     "start_time": "2019-09-04T08:33:34.830072Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and set desired options\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from scipy import sparse, stats\n",
    "from scipy.linalg import svd\n",
    "import umap\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import (KFold, StratifiedKFold, cross_val_score,\n",
    "                                     cross_validate, train_test_split)\n",
    "from tqdm import tqdm\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from code.cross_validation import *\n",
    "from code.read_data import *\n",
    "from code.feature_engineering import *\n",
    "from code.autoencoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T08:33:35.322327Z",
     "start_time": "2019-09-04T08:33:35.320726Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) \n",
    "pd.set_option('display.max_rows', None)\n",
    "sns.set()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T08:33:36.365935Z",
     "start_time": "2019-09-04T08:33:35.903478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X1, X2, X3, Y, X1_test, X2_test, X3_test = read_data()\n",
    "Y = Y.rename(columns={f'{i}': f'target_{i}' for i in range(1, 6)})\n",
    "targets = ['target_1', 'target_2', 'target_3', 'target_4', 'target_5']\n",
    "train_len = len(X1)\n",
    "X2_all = pd.concat([X2, X2_test], ignore_index=True)\n",
    "good_A_labels = set(X2['A'].values) & set(X2_test['A'].values)\n",
    "only_train_A_labels = set(X2['A'].values) - set(X2_test['A'].values)\n",
    "only_test_A_labels = set(X2_test['A'].values) - set(X2['A'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 313 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def aggregate_X2(X2, X2_test):\n",
    "    X2_agg = pd.concat([X2, X2_test], ignore_index=True)\n",
    "    X2_agg['A'] = X2_agg['A'].apply(lambda x: [x])\n",
    "    X2_agg = X2_agg.groupby('id').agg(sum).reset_index()\n",
    "    X2_agg['A'] = X2_agg['A'].apply(lambda x: set(x))\n",
    "    return X2_agg\n",
    "\n",
    "    \n",
    "def get_X2_features(most_frequent_A):\n",
    "    X2_all_cp = aggregate_X2(X2, X2_test)\n",
    "    print(len(most_frequent_A['A'].values))\n",
    "    for item in tqdm(most_frequent_A['A'].values):\n",
    "        X2_all_cp[f'A_feature_{item}'] = X2_all_cp['A'].apply(lambda x: item in x)\n",
    "    X2_all_cp['has_only_train_A_labels'] = X2_all_cp['A'].apply(lambda x: x & only_train_A_labels != set())\n",
    "    X2_all_cp['only_test_A_labels'] = X2_all_cp['A'].apply(lambda x: x & only_test_A_labels != set())\n",
    "    X2_all_cp.drop(columns=['A'], inplace=True)\n",
    "    return X2_all_cp\n",
    "\n",
    "\n",
    "occurences = X2_all.groupby('A')['id'].nunique().reset_index()\n",
    "occurences['good'] = occurences['A'].apply(lambda x: x in good_A_labels)\n",
    "occurences = occurences[occurences['good']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31863/31863 [03:24<00:00, 156.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 6043 samples, validate on 2015 samples\n",
      "Epoch 1/50\n",
      " - 9s - loss: 0.5613 - val_loss: 0.2105\n",
      "Epoch 2/50\n",
      " - 6s - loss: 0.0928 - val_loss: 0.0516\n",
      "Epoch 3/50\n",
      " - 6s - loss: 0.0454 - val_loss: 0.0436\n",
      "Epoch 4/50\n",
      " - 6s - loss: 0.0423 - val_loss: 0.0423\n",
      "Epoch 5/50\n",
      " - 6s - loss: 0.0415 - val_loss: 0.0417\n",
      "Epoch 6/50\n",
      " - 6s - loss: 0.0411 - val_loss: 0.0414\n",
      "Epoch 7/50\n",
      " - 6s - loss: 0.0407 - val_loss: 0.0410\n",
      "Epoch 8/50\n",
      " - 6s - loss: 0.0404 - val_loss: 0.0406\n",
      "Epoch 9/50\n",
      " - 6s - loss: 0.0398 - val_loss: 0.0399\n",
      "Epoch 10/50\n",
      " - 6s - loss: 0.0390 - val_loss: 0.0389\n",
      "Epoch 11/50\n",
      " - 6s - loss: 0.0374 - val_loss: 0.0372\n",
      "Epoch 12/50\n",
      " - 6s - loss: 0.0357 - val_loss: 0.0359\n",
      "Epoch 13/50\n",
      " - 6s - loss: 0.0345 - val_loss: 0.0349\n",
      "Epoch 14/50\n",
      " - 7s - loss: 0.0336 - val_loss: 0.0342\n",
      "Epoch 15/50\n",
      " - 6s - loss: 0.0328 - val_loss: 0.0335\n",
      "Epoch 16/50\n",
      " - 7s - loss: 0.0321 - val_loss: 0.0329\n",
      "Epoch 17/50\n",
      " - 7s - loss: 0.0313 - val_loss: 0.0322\n",
      "Epoch 18/50\n",
      " - 8s - loss: 0.0305 - val_loss: 0.0315\n",
      "Epoch 19/50\n",
      " - 7s - loss: 0.0297 - val_loss: 0.0307\n",
      "Epoch 20/50\n",
      " - 7s - loss: 0.0288 - val_loss: 0.0299\n",
      "Epoch 21/50\n",
      " - 6s - loss: 0.0279 - val_loss: 0.0290\n",
      "Epoch 22/50\n",
      " - 6s - loss: 0.0266 - val_loss: 0.0278\n",
      "Epoch 23/50\n",
      " - 7s - loss: 0.0249 - val_loss: 0.0261\n",
      "Epoch 24/50\n",
      " - 6s - loss: 0.0228 - val_loss: 0.0240\n",
      "Epoch 25/50\n",
      " - 6s - loss: 0.0200 - val_loss: 0.0215\n",
      "Epoch 26/50\n",
      " - 7s - loss: 0.0173 - val_loss: 0.0203\n",
      "Epoch 27/50\n",
      " - 6s - loss: 0.0157 - val_loss: 0.0198\n",
      "Epoch 28/50\n",
      " - 6s - loss: 0.0153 - val_loss: 0.0196\n",
      "Epoch 29/50\n",
      " - 6s - loss: 0.0147 - val_loss: 0.0198\n",
      "Epoch 30/50\n",
      " - 6s - loss: 0.0144 - val_loss: 0.0194\n",
      "Epoch 31/50\n",
      " - 6s - loss: 0.0141 - val_loss: 0.0194\n",
      "Epoch 32/50\n",
      " - 6s - loss: 0.0139 - val_loss: 0.0192\n",
      "Epoch 33/50\n",
      " - 7s - loss: 0.0137 - val_loss: 0.0191\n",
      "Epoch 34/50\n",
      " - 7s - loss: 0.0136 - val_loss: 0.0188\n",
      "Epoch 35/50\n",
      " - 6s - loss: 0.0135 - val_loss: 0.0188\n",
      "Epoch 36/50\n",
      " - 6s - loss: 0.0134 - val_loss: 0.0185\n",
      "Epoch 37/50\n",
      " - 6s - loss: 0.0132 - val_loss: 0.0184\n",
      "Epoch 38/50\n",
      " - 6s - loss: 0.0131 - val_loss: 0.0182\n",
      "Epoch 39/50\n",
      " - 6s - loss: 0.0130 - val_loss: 0.0181\n",
      "Epoch 40/50\n",
      " - 6s - loss: 0.0129 - val_loss: 0.0181\n",
      "Epoch 41/50\n",
      " - 6s - loss: 0.0128 - val_loss: 0.0179\n",
      "Epoch 42/50\n",
      " - 6s - loss: 0.0127 - val_loss: 0.0178\n",
      "Epoch 43/50\n",
      " - 7s - loss: 0.0127 - val_loss: 0.0177\n",
      "Epoch 44/50\n",
      " - 6s - loss: 0.0126 - val_loss: 0.0176\n",
      "Epoch 45/50\n",
      " - 6s - loss: 0.0125 - val_loss: 0.0176\n",
      "Epoch 46/50\n",
      " - 6s - loss: 0.0124 - val_loss: 0.0175\n",
      "Epoch 47/50\n",
      " - 6s - loss: 0.0124 - val_loss: 0.0175\n",
      "Epoch 48/50\n",
      " - 6s - loss: 0.0123 - val_loss: 0.0174\n",
      "Epoch 49/50\n",
      " - 7s - loss: 0.0122 - val_loss: 0.0173\n",
      "Epoch 50/50\n",
      " - 7s - loss: 0.0122 - val_loss: 0.0173\n",
      "Wall time: 8min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# autoencoder\n",
    "X2_all_large = get_X2_features(occurences[occurences['id'] >= 4])\n",
    "ids = X2_all_large['id']\n",
    "X2_all_encoded = encode(X2_all_large.drop(columns=['id']), encoding_dim=32)\n",
    "X2_all_encoded['id'] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# X2 lots of features\n",
    "X2_agg = aggregate_X2(X2, X2_test)\n",
    "X2_agg['good_features_count'] = X2_agg['A'].apply(lambda x: len(x & good_A_labels))\n",
    "X2_agg['only_train_A_labels_count'] = X2_agg['A'].apply(lambda x: len(x & only_train_A_labels))\n",
    "X2_agg['only_test_A_labels_count'] = X2_agg['A'].apply(lambda x: len(x & only_test_A_labels))\n",
    "X2_agg['A_label_sum_for_ratio'] = X2_agg['good_features_count'] + X2_agg['only_train_A_labels_count'] + X2_agg['only_test_A_labels_count']\n",
    "X2_agg['good_features_count_ratio'] = X2_agg['good_features_count'] / X2_agg['A_label_sum_for_ratio']\n",
    "X2_agg['only_train_A_labels_count'] = X2_agg['only_train_A_labels_count'] / X2_agg['A_label_sum_for_ratio']\n",
    "X2_agg['only_test_A_labels_count'] = X2_agg['only_test_A_labels_count'] / X2_agg['A_label_sum_for_ratio']\n",
    "X2_agg.drop(columns=['A'], inplace=True)\n",
    "X2_agg.fillna(0, inplace=True)\n",
    "X2_features = pd.merge(X2_all_encoded, X2_agg, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31863/31863 [03:36<00:00, 147.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 52min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# X1\n",
    "X1_all = pd.concat([X1, X1_test], ignore_index=True)\n",
    "bool_columns = [col for col in X1_all.columns if X1_all[col].nunique() == 2]\n",
    "not_bool_columns = list(set(X1_all.columns) - set(bool_columns) - set(['id']))\n",
    "# X1 categorical\n",
    "X1_categorical_embedding = umap.UMAP(metric='dice').fit_transform(X1_all[bool_columns])\n",
    "X1_categorical_embedding_df = pd.DataFrame(X1_categorical_embedding, columns=['X1_all_embedding_1', 'X1_all_embedding_2'])\n",
    "X1_categorical_embedding_df['id'] = X1_all['id']\n",
    "# X1 other\n",
    "X1_other_embedding = umap.UMAP().fit_transform(X1_all[not_bool_columns])\n",
    "X1_other_embedding_df = pd.DataFrame(X1_other_embedding, columns=['X1_all_embedding_3', 'X1_all_embedding_4'])\n",
    "X1_other_embedding_df['id'] = X1_all['id']\n",
    "# X2\n",
    "X2_all_cp = get_X2_features(occurences[occurences['id'] >= 4])\n",
    "X2_all_embedding = umap.UMAP(metric='dice').fit_transform(X2_all_cp.drop(columns=['id']))\n",
    "X2_all_embedding_df = pd.DataFrame(X2_all_embedding, columns=['X2_all_embedding_1', 'X2_all_embedding_2'])\n",
    "X2_all_embedding_df['id'] = X2_all_cp['id']\n",
    "# unite\n",
    "embeddings = [X1_categorical_embedding_df, X1_other_embedding_df, X2_all_embedding_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/embeddings_4.pickle','wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "umap...\n",
      "Wall time: 56.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = agg_and_merge(X1, X2, X3, embeddings)\n",
    "X_test = agg_and_merge(X1_test, X2_test, X3_test, embeddings)\n",
    "X_train, X_test = eng(X_train, X_test)\n",
    "X_train, X_test = add_emedding_features(X_train, X_test, random_state=42)\n",
    "X_train_norm, X_test_norm = normalize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 365 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_norm = pd.merge(X_train_norm, X2_features, on='id')\n",
    "X_test_norm = pd.merge(X_test_norm, X2_features, on='id')\n",
    "X_train = pd.merge(X_train, X2_features, on='id')\n",
    "X_test = pd.merge(X_test, X2_features, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_norm_ext = pd.merge(X_train_norm, X2_all_large, on='id')\n",
    "X_test_norm_ext = pd.merge(X_test_norm, X2_all_large, on='id')\n",
    "X_train_ext = pd.merge(X_train, X2_all_large, on='id')\n",
    "X_test_ext = pd.merge(X_test, X2_all_large, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "et_multi = ExtraTreesClassifier(n_estimators=1000,\n",
    "                                max_depth=7,\n",
    "                                class_weight='balanced',\n",
    "                                random_state=42,\n",
    "                                n_jobs=4)\n",
    "#CV_multilabel(et_multi, X_train_ext, Y.drop(columns=['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_multi = RandomForestClassifier(n_estimators=1000,\n",
    "                                  max_depth=10,\n",
    "                                  class_weight='balanced',\n",
    "                                  random_state=42,\n",
    "                                  n_jobs=4)\n",
    "#CV_multilabel(rf_multi, X_train_ext, Y.drop(columns=['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "step = 500\n",
    "intervals = [(i, i + step) for i in range(0, 4000, step)]\n",
    "splits = [[item for j, item in enumerate(intervals) if i != j] for i in range(len(intervals))]\n",
    "out_dims = [[item for j, item in enumerate(intervals) if i == j] for i in range(len(intervals))]\n",
    "X_train_splits = [pd.concat([X_train_ext[lb:ub] for (lb, ub) in split]) for split in splits]\n",
    "y_train_splits = [pd.concat([Y[lb:ub] for (lb, ub) in split]) for split in splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\n",
      "1it [03:10, 190.44s/it]\n",
      "2it [05:55, 182.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "metafeatures = pd.concat([X_train[['id']], X_test[['id']]], ignore_index=True)\n",
    "for model_name, model in tqdm(zip(['rf_multi', 'et_multi'], [rf_multi, et_multi])):\n",
    "    X_train_predicts = []\n",
    "    X_test_predicts = []\n",
    "    for X_train_split, y_train_split, out_dim in zip(X_train_splits, y_train_splits, out_dims):\n",
    "        model.fit(X_train_split, y_train_split.drop(columns=['id']))\n",
    "        X_train_predicts.append(model.predict_proba(X_train_ext[out_dim[0][0]:out_dim[0][1]]))\n",
    "        X_test_predicts.append(model.predict_proba(X_test_ext))\n",
    "    \n",
    "    # X train features\n",
    "    X_train_features = {target: [] for target in targets}\n",
    "    for chunk in X_train_predicts:\n",
    "        for target, probas in zip(targets, chunk):\n",
    "            X_train_features[target].append(probas[:, 1])\n",
    "    for target in targets:\n",
    "        X_train_features[target] = np.concatenate(X_train_features[target])\n",
    "    # X test features\n",
    "    X_test_features = {target: [] for target in targets}\n",
    "    for chunk in X_test_predicts:\n",
    "        for target, probas in zip(targets, chunk):\n",
    "            X_test_features[target].append(probas[:, 1])\n",
    "    for target in targets:\n",
    "        X_test_features[target] = sum(X_test_features[target]) / len(X_test_features[target])\n",
    "    # metafeatures\n",
    "    for target in targets:\n",
    "        metafeatures[f'meta_{model_name}_{target}'] = np.concatenate((X_train_features[target], X_test_features[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 365 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_norm = pd.merge(X_train_norm, metafeatures, on='id')\n",
    "X_test_norm = pd.merge(X_test_norm, metafeatures, on='id')\n",
    "X_train = pd.merge(X_train, metafeatures, on='id')\n",
    "X_test = pd.merge(X_test, metafeatures, on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T08:33:37.485178Z",
     "start_time": "2019-09-04T08:33:37.478886Z"
    }
   },
   "source": [
    "# Simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(X_train, Y, on='id')\n",
    "logreg_merged = pd.merge(X_train_norm, Y, on='id')\n",
    "assert len(merged) == len(logreg_merged)\n",
    "final_models = {}\n",
    "final_models_roc_auc = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logregs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc  avg:  0.588   ['0.576', '0.568', '0.589', '0.585', '0.624']\n",
      "roc_auc  avg:  0.618   ['0.620', '0.613', '0.639', '0.597', '0.619']\n",
      "roc_auc  avg:  0.615   ['0.585', '0.611', '0.633', '0.658', '0.588']\n",
      "roc_auc  avg:  0.603   ['0.630', '0.623', '0.609', '0.574', '0.581']\n",
      "roc_auc  avg:  0.610   ['0.625', '0.624', '0.610', '0.632', '0.559']\n",
      "0.6069017423916294\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logreg = LogisticRegression(C=0.05, class_weight='balanced', random_state=42, n_jobs=4)\n",
    "roc_aucs = []\n",
    "for target in targets:\n",
    "    roc_aucs.append(CV_metrics(logreg, X_train_norm, Y[target].values))\n",
    "print(np.mean(roc_aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc  avg:  0.592   ['0.552', '0.579', '0.592', '0.593', '0.645']\n",
      "roc_auc  avg:  0.619   ['0.623', '0.636', '0.631', '0.597', '0.610']\n",
      "roc_auc  avg:  0.615   ['0.603', '0.609', '0.624', '0.629', '0.610']\n",
      "roc_auc  avg:  0.606   ['0.626', '0.617', '0.613', '0.581', '0.595']\n",
      "roc_auc  avg:  0.602   ['0.595', '0.625', '0.608', '0.621', '0.558']\n",
      "0.6068022973124194\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier(n_estimators=1000,\n",
    "                            max_depth=6,\n",
    "                            class_weight='balanced',\n",
    "                            random_state=42,\n",
    "                            n_jobs=4)\n",
    "roc_aucs = []\n",
    "for target in targets:\n",
    "    roc_aucs.append(CV_metrics(rf, X_train, Y[target].values))\n",
    "print(np.mean(roc_aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc  avg:  0.565   ['0.512', '0.548', '0.564', '0.585', '0.620']\n",
      "roc_auc  avg:  0.618   ['0.627', '0.633', '0.630', '0.602', '0.600']\n",
      "roc_auc  avg:  0.618   ['0.599', '0.611', '0.626', '0.642', '0.612']\n",
      "roc_auc  avg:  0.609   ['0.636', '0.620', '0.614', '0.589', '0.589']\n",
      "roc_auc  avg:  0.596   ['0.592', '0.625', '0.589', '0.616', '0.560']\n",
      "0.6014856557402766\n",
      "Wall time: 40.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "et = ExtraTreesClassifier(n_estimators=1000,\n",
    "                          max_depth=9,\n",
    "                          class_weight='balanced',\n",
    "                          random_state=42,\n",
    "                          n_jobs=4)\n",
    "roc_aucs = []\n",
    "for target in targets:\n",
    "    roc_aucs.append(CV_metrics(et, X_train, Y[target].values))\n",
    "print(np.mean(roc_aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc  avg:  0.611   ['0.591', '0.599', '0.612', '0.607', '0.648']\n",
      "roc_auc  avg:  0.620   ['0.623', '0.636', '0.632', '0.598', '0.610']\n",
      "roc_auc  avg:  0.624   ['0.605', '0.613', '0.649', '0.632', '0.620']\n",
      "roc_auc  avg:  0.601   ['0.621', '0.609', '0.623', '0.573', '0.581']\n",
      "roc_auc  avg:  0.603   ['0.603', '0.625', '0.599', '0.622', '0.563']\n",
      "0.6117977038250448\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gbm = LGBMClassifier(num_leaves=3,\n",
    "                     learning_rate=0.05,\n",
    "                     reg_lambda=75.0,\n",
    "                     random_state=42,\n",
    "                     class_weight='balanced')\n",
    "roc_aucs = []\n",
    "for target in targets:\n",
    "    roc_aucs.append(CV_metrics(gbm, X_train, Y[target].values))\n",
    "print(np.mean(roc_aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_1\n",
      "target_2\n",
      "target_3\n",
      "target_4\n",
      "target_5\n",
      "Wall time: 37.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "probas = []\n",
    "X_train = merged.drop(columns=targets)\n",
    "X_train_logreg = logreg_merged.drop(columns=targets)\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    y_train = merged[target].values\n",
    "    # models\n",
    "    # lightgbm\n",
    "    y_proba = gbm.fit(X_train, y_train).predict_proba(X_test)[:, 1]\n",
    "    # random forest\n",
    "    y_proba += rf.fit(X_train, y_train).predict_proba(X_test)[:, 1]\n",
    "    # extra tree\n",
    "    y_proba += et.fit(X_train, y_train).predict_proba(X_test)[:, 1]\n",
    "    # logreg\n",
    "    y_proba += logreg.fit(X_train_norm, y_train).predict_proba(X_test_norm)[:, 1]\n",
    "    y_proba /= 4.0\n",
    "    final_proba = y_proba\n",
    "    probas.append(final_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(probas).T\n",
    "baseline = pd.DataFrame(tmp.values, columns=['1', '2', '3', '4', '5'])\n",
    "baseline['id'] = X_test['id']\n",
    "baseline[['id', '1', '2', '3', '4', '5']].to_csv('baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "246.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
