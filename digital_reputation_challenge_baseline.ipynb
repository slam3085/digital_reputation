{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T08:33:35.319634Z",
     "start_time": "2019-09-04T08:33:34.830072Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and set desired options\n",
    "from itertools import permutations\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from scipy import sparse, stats\n",
    "from scipy.linalg import svd\n",
    "import umap\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import (KFold, StratifiedKFold, cross_val_score,\n",
    "                                     cross_validate, train_test_split)\n",
    "from tqdm import tqdm\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from code.cross_validation import *\n",
    "from code.read_data import *\n",
    "from code.feature_engineering import *\n",
    "from code.autoencoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T08:33:35.322327Z",
     "start_time": "2019-09-04T08:33:35.320726Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) \n",
    "pd.set_option('display.max_rows', None)\n",
    "sns.set()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T08:33:36.365935Z",
     "start_time": "2019-09-04T08:33:35.903478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 919 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X1, X2, X3, Y, X1_test, X2_test, X3_test = read_data()\n",
    "targets = [col for col in Y.columns if col != 'id']\n",
    "train_len = len(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with X2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def aggregate_X2(X2, X2_test):\n",
    "    X2_agg = pd.concat([X2, X2_test], ignore_index=True)\n",
    "    X2_agg['A'] = X2_agg['A'].apply(lambda x: [x])\n",
    "    X2_agg = X2_agg.groupby('id').agg(sum).reset_index()\n",
    "    X2_agg['A'] = X2_agg['A'].apply(lambda x: set(x))\n",
    "    return X2_agg\n",
    "\n",
    "\n",
    "def X2_freq_features(X2, X2_test):\n",
    "    X2_all = pd.concat([X2, X2_test], ignore_index=True)\n",
    "    A_mapping_df = X2_all.groupby('A')['id'].nunique().reset_index().rename(columns={'id': 'freq'})\n",
    "    A_mapping_df['freq'] = A_mapping_df['freq'] / X2_all['id'].nunique()\n",
    "    A_mapping = {k: v for k, v in zip(A_mapping_df['A'].values, A_mapping_df['freq'].values)}\n",
    "    X2_agg = aggregate_X2(X2, X2_test)\n",
    "    X2_agg['A'] = X2_agg['A'].apply(lambda x: np.array([A_mapping[item] for item in x]))\n",
    "    X2_agg['min_freq_A'] = X2_agg['A'].apply(lambda x: x.min())\n",
    "    X2_agg['max_freq_A'] = X2_agg['A'].apply(lambda x: x.max())\n",
    "    X2_agg['mean_freq_A'] = X2_agg['A'].apply(lambda x: x.mean())\n",
    "    X2_agg['median_freq_A'] = X2_agg['A'].apply(lambda x: np.median(x))\n",
    "    X2_agg['var_freq_A'] = X2_agg['A'].apply(lambda x: x.var())\n",
    "    X2_agg['min_max_freq_ratio_A'] = X2_agg['min_freq_A'] / X2_agg['max_freq_A']\n",
    "    X2_agg['mean_median_freq_delta_A'] = np.abs((X2_agg['mean_freq_A'] - X2_agg['median_freq_A']) / X2_agg['mean_freq_A'])\n",
    "    return X2_agg.drop(columns=['A'])\n",
    "\n",
    "\n",
    "X2_freq = X2_freq_features(X2, X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 821 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# keep categories existing in both train and test\n",
    "good_A_labels = set(X2['A'].values) & set(X2_test['A'].values)\n",
    "X2['A'] = X2['A'].apply(lambda x: x if x in good_A_labels else -1)\n",
    "X2_test['A'] = X2_test['A'].apply(lambda x: x if x in good_A_labels else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53116"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2_all = pd.concat([X2, X2_test], ignore_index=True)\n",
    "len(set(X2_all['A'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 190 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_X2_features(most_frequent_A):\n",
    "    X2_all_cp = aggregate_X2(X2, X2_test)\n",
    "    print(len(most_frequent_A['A'].values))\n",
    "    for item in tqdm(most_frequent_A['A'].values):\n",
    "        X2_all_cp[f'A_feature_{item}'] = X2_all_cp['A'].apply(lambda x: item in x)\n",
    "    X2_all_cp.drop(columns=['A'], inplace=True)\n",
    "    return X2_all_cp\n",
    "\n",
    "\n",
    "occurences = X2_all.groupby('A')['id'].nunique().reset_index()\n",
    "occurences['good'] = occurences['A'].apply(lambda x: x in good_A_labels)\n",
    "occurences = occurences[occurences['good']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53115/53115 [08:27<00:00, 104.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# autoencoder\n",
    "X2_all_large = get_X2_features(occurences[occurences['id'] >= 1])\n",
    "\n",
    "\n",
    "#ids = X2_all_large['id']\n",
    "#X2_all_encoded = encode(X2_all_large.drop(columns=['id']), encoding_dim=32)\n",
    "#X2_all_encoded['id'] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26508/26508 [02:32<00:00, 174.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 42min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# X1\n",
    "X1_all = pd.concat([X1, X1_test], ignore_index=True)\n",
    "bool_columns = [col for col in X1_all.columns if X1_all[col].nunique() == 2]\n",
    "not_bool_columns = list(set(X1_all.columns) - set(bool_columns) - set(['id']))\n",
    "# X1 categorical\n",
    "X1_categorical_embedding = umap.UMAP(n_components=3, metric='dice').fit_transform(X1_all[bool_columns])\n",
    "X1_categorical_embedding_df = pd.DataFrame(X1_categorical_embedding, columns=[f'X1_all_embedding_{i+1}' for i in range(3)])\n",
    "X1_categorical_embedding_df['id'] = X1_all['id']\n",
    "# X1 other\n",
    "X1_other_embedding = umap.UMAP().fit_transform(X1_all[not_bool_columns])\n",
    "X1_other_embedding_df = pd.DataFrame(X1_other_embedding, columns=['X1_all_embedding_5', 'X1_all_embedding_6'])\n",
    "X1_other_embedding_df['id'] = X1_all['id']\n",
    "# X2\n",
    "X2_all_cp = get_X2_features(occurences[occurences['id'] >= 5])\n",
    "X2_all_embedding = umap.UMAP(n_components=32, metric='dice').fit_transform(X2_all_cp.drop(columns=['id']))\n",
    "X2_all_embedding_df = pd.DataFrame(X2_all_embedding, columns=[f'X2_all_embedding_{i+1}' for i in range(32)])\n",
    "X2_all_embedding_df['id'] = X2_all_cp['id']\n",
    "# unite\n",
    "embeddings = [X1_categorical_embedding_df, X1_other_embedding_df, X2_all_embedding_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/embeddings_5_n.pickle','wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.append(X2_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "umap...\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = agg_and_merge(X1, X2, X3, embeddings)\n",
    "X_test = agg_and_merge(X1_test, X2_test, X3_test, embeddings)\n",
    "X_train, X_test = eng(X_train, X_test)\n",
    "X_train, X_test = add_emedding_features(X_train, X_test, random_state=42)\n",
    "X_train_norm, X_test_norm = normalize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 309 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_norm = pd.merge(X_train_norm, X2_all_encoded, on='id')\n",
    "X_test_norm = pd.merge(X_test_norm, X2_all_encoded, on='id')\n",
    "X_train = pd.merge(X_train, X2_all_encoded, on='id')\n",
    "X_test = pd.merge(X_test, X2_all_encoded, on='id')\n",
    "assert len(X_train) == 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_norm_ext = pd.merge(X_train_norm, X2_all_large, on='id')\n",
    "X_test_norm_ext = pd.merge(X_test_norm, X2_all_large, on='id')\n",
    "X_train_ext = pd.merge(X_train, X2_all_large, on='id')\n",
    "X_test_ext = pd.merge(X_test, X2_all_large, on='id')\n",
    "assert len(X_train_norm_ext) == 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_1: 0.547, [0.5517395499304748, 0.5299615338382018, 0.5181210936342902, 0.5636546229886608, 0.5731555967582763]\n",
      "target_2: 0.609, [0.5989356387555161, 0.5905393778064585, 0.6118233766233766, 0.6113504464285714, 0.6327650948273517]\n",
      "target_3: 0.604, [0.5855873358517719, 0.6078719194099659, 0.6085276884191176, 0.6153430687487763, 0.6045316808363971]\n",
      "target_4: 0.598, [0.6006241565452092, 0.5902395915161037, 0.6368010160763714, 0.5708417374086634, 0.5894440630452816]\n",
      "target_5: 0.567, [0.5439161966156325, 0.5513727517618568, 0.59996, 0.539924248824308, 0.5976504040706375]\n",
      "0.5849872876288508\n",
      "Wall time: 56.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "et_multi = ExtraTreesClassifier(n_estimators=100,\n",
    "                                max_depth=7,\n",
    "                                class_weight='balanced',\n",
    "                                random_state=42,\n",
    "                                n_jobs=4)\n",
    "CV_multilabel(et_multi, X_train_ext, Y.drop(columns=['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_1: 0.546, [0.5533570759669684, 0.5515010818608006, 0.500118486922006, 0.5593752576237906, 0.5644980412093663]\n",
      "target_2: 0.622, [0.6125766700896451, 0.6128834933182759, 0.6112, 0.6344940476190476, 0.6400710184756877]\n",
      "target_3: 0.601, [0.5730742939377587, 0.6283432271991365, 0.593240176930147, 0.5808437810139144, 0.6287482766544118]\n",
      "target_4: 0.602, [0.6005704821494295, 0.6001500953877231, 0.6244797095346895, 0.5957547413761258, 0.5886610028890986]\n",
      "target_5: 0.593, [0.5818159657094102, 0.5870860656852874, 0.6055000000000001, 0.5872968939201937, 0.6014666267584556]\n",
      "0.5926842604892549\n",
      "Wall time: 1min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_multi = RandomForestClassifier(n_estimators=1000,\n",
    "                                  max_depth=10,\n",
    "                                  class_weight='balanced',\n",
    "                                  random_state=42,\n",
    "                                  n_jobs=4)\n",
    "CV_multilabel(rf_multi, X_train_ext, Y.drop(columns=['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "step = 500\n",
    "intervals = [(i, i + step) for i in range(0, 4000, step)]\n",
    "splits = [[item for j, item in enumerate(intervals) if i != j] for i in range(len(intervals))]\n",
    "out_dims = [[item for j, item in enumerate(intervals) if i == j] for i in range(len(intervals))]\n",
    "X_train_splits = [pd.concat([X_train_ext[lb:ub] for (lb, ub) in split]) for split in splits]\n",
    "y_train_splits = [pd.concat([Y[lb:ub] for (lb, ub) in split]) for split in splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [04:53, 152.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "metafeatures = pd.concat([X_train[['id']], X_test[['id']]], ignore_index=True)\n",
    "for model_name, model in tqdm(zip(['rf_multi', 'et_multi'], [rf_multi, et_multi])):\n",
    "    X_train_predicts = []\n",
    "    X_test_predicts = []\n",
    "    for X_train_split, y_train_split, out_dim in zip(X_train_splits, y_train_splits, out_dims):\n",
    "        model.fit(X_train_split, y_train_split.drop(columns=['id']))\n",
    "        X_train_predicts.append(model.predict_proba(X_train_ext[out_dim[0][0]:out_dim[0][1]]))\n",
    "        X_test_predicts.append(model.predict_proba(X_test_ext))\n",
    "    \n",
    "    # X train features\n",
    "    X_train_features = {target: [] for target in targets}\n",
    "    for chunk in X_train_predicts:\n",
    "        for target, probas in zip(targets, chunk):\n",
    "            X_train_features[target].append(probas[:, 1])\n",
    "    for target in targets:\n",
    "        X_train_features[target] = np.concatenate(X_train_features[target])\n",
    "    # X test features\n",
    "    X_test_features = {target: [] for target in targets}\n",
    "    for chunk in X_test_predicts:\n",
    "        for target, probas in zip(targets, chunk):\n",
    "            X_test_features[target].append(probas[:, 1])\n",
    "    for target in targets:\n",
    "        X_test_features[target] = sum(X_test_features[target]) / len(X_test_features[target])\n",
    "    # metafeatures\n",
    "    for target in targets:\n",
    "        metafeatures[f'meta_{model_name}_{target}'] = np.concatenate((X_train_features[target], X_test_features[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 169 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_norm = pd.merge(X_train_norm, metafeatures, on='id')\n",
    "X_test_norm = pd.merge(X_test_norm, metafeatures, on='id')\n",
    "X_train = pd.merge(X_train, metafeatures, on='id')\n",
    "X_test = pd.merge(X_test, metafeatures, on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T08:33:37.485178Z",
     "start_time": "2019-09-04T08:33:37.478886Z"
    }
   },
   "source": [
    "# Simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(X_train, Y, on='id')\n",
    "logreg_merged = pd.merge(X_train_norm, Y, on='id')\n",
    "assert len(merged) == len(logreg_merged)\n",
    "final_models = {}\n",
    "final_models_roc_auc = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logregs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg: 0.597, delta: 0.068, std: 0.023\n",
      "avg: 0.625, delta: 0.036, std: 0.014\n",
      "avg: 0.619, delta: 0.054, std: 0.022\n",
      "avg: 0.613, delta: 0.057, std: 0.021\n",
      "avg: 0.619, delta: 0.052, std: 0.019\n",
      "0.6144235644664617\n",
      "Wall time: 9.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logreg = LogisticRegression(C=0.04, class_weight='balanced', random_state=42, n_jobs=4)\n",
    "roc_aucs = []\n",
    "for target in targets:\n",
    "    roc_aucs.append(CV_metrics(logreg, X_train_norm, Y[target].values))\n",
    "print(np.mean(roc_aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg: 0.615, delta: 0.064, std: 0.022\n",
      "avg: 0.623, delta: 0.032, std: 0.014\n",
      "avg: 0.621, delta: 0.032, std: 0.011\n",
      "avg: 0.618, delta: 0.041, std: 0.017\n",
      "avg: 0.609, delta: 0.061, std: 0.021\n",
      "0.6170389950011488\n",
      "Wall time: 4min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier(n_estimators=1000,\n",
    "                            max_depth=4,\n",
    "                            min_samples_leaf=50,\n",
    "                            max_features=0.2,\n",
    "                            oob_score=True,\n",
    "                            class_weight='balanced',\n",
    "                            random_state=42,\n",
    "                            n_jobs=4)\n",
    "roc_aucs = []\n",
    "for target in targets:\n",
    "    roc_aucs.append(CV_metrics(rf, X_train, Y[target].values))\n",
    "print(np.mean(roc_aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg: 0.600, delta: 0.069, std: 0.025\n",
      "avg: 0.625, delta: 0.036, std: 0.015\n",
      "avg: 0.621, delta: 0.045, std: 0.018\n",
      "avg: 0.622, delta: 0.052, std: 0.019\n",
      "avg: 0.602, delta: 0.048, std: 0.018\n",
      "0.614140259114123\n",
      "Wall time: 22.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "et = ExtraTreesClassifier(n_estimators=100,\n",
    "                          max_depth=8,\n",
    "                          min_samples_leaf=50,\n",
    "                          max_features=0.25,\n",
    "                          class_weight='balanced',\n",
    "                          random_state=42,\n",
    "                          n_jobs=4)\n",
    "roc_aucs = []\n",
    "for target in targets:\n",
    "    roc_aucs.append(CV_metrics(et, X_train, Y[target].values))\n",
    "print(np.mean(roc_aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg: 0.611, delta: 0.053, std: 0.019\n",
      "avg: 0.621, delta: 0.036, std: 0.014\n",
      "avg: 0.633, delta: 0.054, std: 0.019\n",
      "avg: 0.617, delta: 0.043, std: 0.018\n",
      "avg: 0.608, delta: 0.064, std: 0.022\n",
      "0.6179262525091284\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gbm = LGBMClassifier(num_leaves=3,\n",
    "                     learning_rate=0.055,\n",
    "                     reg_lambda=75.0,\n",
    "                     random_state=42,\n",
    "                     class_weight='balanced')\n",
    "roc_aucs = []\n",
    "for target in targets:\n",
    "    roc_aucs.append(CV_metrics(gbm, X_train, Y[target].values))\n",
    "print(np.mean(roc_aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_1\n",
      "target_2\n",
      "target_3\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "probas = []\n",
    "X_train = merged.drop(columns=targets)\n",
    "X_train_logreg = logreg_merged.drop(columns=targets)\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    y_train = merged[target].values\n",
    "    # models\n",
    "    # lightgbm\n",
    "    y_proba = gbm.fit(X_train, y_train).predict_proba(X_test)[:, 1]\n",
    "    # random forest\n",
    "    y_proba += rf.fit(X_train, y_train).predict_proba(X_test)[:, 1]\n",
    "    # extra tree\n",
    "    y_proba += et.fit(X_train, y_train).predict_proba(X_test)[:, 1]\n",
    "    # logreg\n",
    "    y_proba += logreg.fit(X_train_norm, y_train).predict_proba(X_test_norm)[:, 1]\n",
    "    y_proba /= 4.0\n",
    "    final_proba = y_proba\n",
    "    probas.append(final_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(probas).T\n",
    "baseline = pd.DataFrame(tmp.values, columns=['1', '2', '3', '4', '5'])\n",
    "baseline['id'] = X_test['id']\n",
    "baseline[['id', '1', '2', '3', '4', '5']].to_csv('baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "246.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
