{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T08:33:35.319634Z",
     "start_time": "2019-09-04T08:33:34.830072Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries and set desired options\n",
    "from itertools import permutations\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from scipy import sparse, stats\n",
    "from scipy.linalg import svd\n",
    "import umap\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import (KFold, StratifiedKFold, cross_val_score,\n",
    "                                     cross_validate, train_test_split)\n",
    "from tqdm import tqdm\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from code.cross_validation import *\n",
    "from code.read_data import *\n",
    "from code.feature_engineering import *\n",
    "from code.autoencoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T08:33:35.322327Z",
     "start_time": "2019-09-04T08:33:35.320726Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) \n",
    "pd.set_option('display.max_rows', None)\n",
    "sns.set()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T08:33:36.365935Z",
     "start_time": "2019-09-04T08:33:35.903478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 910 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X1, X2, X3, Y, X1_test, X2_test, X3_test = read_data()\n",
    "targets = [col for col in Y.columns if col != 'id']\n",
    "train_len = len(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with X2 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 730 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# keep categories existing in both train and test\n",
    "good_A_labels = set(X2['A'].values) & set(X2_test['A'].values)\n",
    "X2['A'] = X2['A'].apply(lambda x: x if x in good_A_labels else -1)\n",
    "X2_test['A'] = X2_test['A'].apply(lambda x: x if x in good_A_labels else -1)\n",
    "# compress class categories\n",
    "#def perm(arr):\n",
    "#    return set([''.join(list(str(i) for i in item)) for item in permutations(arr)])\n",
    "#\n",
    "#all_possible_targets = perm([0, 0, 0, 0, 0]) | \\\n",
    "#                       perm([0, 0, 0, 0, 1]) | \\\n",
    "#                       perm([0, 0, 0, 1, 1]) | \\\n",
    "#                       perm([0, 0, 1, 1, 1]) | \\\n",
    "#                       perm([0, 1, 1, 1, 1]) | \\\n",
    "#                       perm([1, 1, 1, 1, 1])\n",
    "#tmp = pd.merge(X2, Y, how='left', on='id')\n",
    "#tmp['concat'] = tmp['target_1'].astype(str) + \\\n",
    "#                tmp['target_2'].astype(str) + \\\n",
    "#                tmp['target_3'].astype(str) + \\\n",
    "#                tmp['target_4'].astype(str) + \\\n",
    "#                tmp['target_5'].astype(str)\n",
    "#for i, target in enumerate(all_possible_targets):\n",
    "#    can_be_compressed = set(tmp[tmp['concat'] == target]['A'].values) - set(tmp[tmp['concat'] != target]['A'].values)\n",
    "#    X2['A'] = X2['A'].apply(lambda x: x if x not in can_be_compressed else -(i + 1))\n",
    "#    X2_test['A'] = X2_test['A'].apply(lambda x: x if x not in can_be_compressed else -(i + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53116"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2_all = pd.concat([X2, X2_test], ignore_index=True)\n",
    "len(set(X2_all['A'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aggregate_X2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fc5bcfff615a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msortedcontainers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSortedSet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX2_agg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maggregate_X2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX2_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX2_agg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'A'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX2_agg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'A'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSortedSet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#X2_agg['A'].values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'aggregate_X2' is not defined"
     ]
    }
   ],
   "source": [
    "from sortedcontainers import SortedSet\n",
    "X2_agg = aggregate_X2(X2, X2_test)\n",
    "X2_agg['A'] = X2_agg['A'].apply(lambda x: SortedSet(x))\n",
    "#X2_agg['A'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 188 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def aggregate_X2(X2, X2_test):\n",
    "    X2_agg = pd.concat([X2, X2_test], ignore_index=True)\n",
    "    X2_agg['A'] = X2_agg['A'].apply(lambda x: [x])\n",
    "    X2_agg = X2_agg.groupby('id').agg(sum).reset_index()\n",
    "    X2_agg['A'] = X2_agg['A'].apply(lambda x: set(x))\n",
    "    return X2_agg\n",
    "\n",
    "    \n",
    "def get_X2_features(most_frequent_A):\n",
    "    X2_all_cp = aggregate_X2(X2, X2_test)\n",
    "    print(len(most_frequent_A['A'].values))\n",
    "    for item in tqdm(most_frequent_A['A'].values):\n",
    "        X2_all_cp[f'A_feature_{item}'] = X2_all_cp['A'].apply(lambda x: item in x)\n",
    "    X2_all_cp.drop(columns=['A'], inplace=True)\n",
    "    return X2_all_cp\n",
    "\n",
    "\n",
    "occurences = X2_all.groupby('A')['id'].nunique().reset_index()\n",
    "occurences['good'] = occurences['A'].apply(lambda x: x in good_A_labels)\n",
    "occurences = occurences[occurences['good']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26508/26508 [02:33<00:00, 172.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 6043 samples, validate on 2015 samples\n",
      "Epoch 1/50\n",
      " - 9s - loss: 0.5480 - val_loss: 0.2008\n",
      "Epoch 2/50\n",
      " - 5s - loss: 0.1012 - val_loss: 0.0610\n",
      "Epoch 3/50\n",
      " - 5s - loss: 0.0543 - val_loss: 0.0512\n",
      "Epoch 4/50\n",
      " - 5s - loss: 0.0496 - val_loss: 0.0492\n",
      "Epoch 5/50\n",
      " - 5s - loss: 0.0485 - val_loss: 0.0485\n",
      "Epoch 6/50\n",
      " - 5s - loss: 0.0480 - val_loss: 0.0481\n",
      "Epoch 7/50\n",
      " - 5s - loss: 0.0476 - val_loss: 0.0477\n",
      "Epoch 8/50\n",
      " - 5s - loss: 0.0471 - val_loss: 0.0472\n",
      "Epoch 9/50\n",
      " - 5s - loss: 0.0465 - val_loss: 0.0463\n",
      "Epoch 10/50\n",
      " - 5s - loss: 0.0452 - val_loss: 0.0450\n",
      "Epoch 11/50\n",
      " - 5s - loss: 0.0435 - val_loss: 0.0436\n",
      "Epoch 12/50\n",
      " - 5s - loss: 0.0419 - val_loss: 0.0424\n",
      "Epoch 13/50\n",
      " - 5s - loss: 0.0408 - val_loss: 0.0415\n",
      "Epoch 14/50\n",
      " - 5s - loss: 0.0398 - val_loss: 0.0407\n",
      "Epoch 15/50\n",
      " - 5s - loss: 0.0389 - val_loss: 0.0399\n",
      "Epoch 16/50\n",
      " - 5s - loss: 0.0379 - val_loss: 0.0392\n",
      "Epoch 17/50\n",
      " - 5s - loss: 0.0371 - val_loss: 0.0384\n",
      "Epoch 18/50\n",
      " - 5s - loss: 0.0361 - val_loss: 0.0376\n",
      "Epoch 19/50\n",
      " - 5s - loss: 0.0351 - val_loss: 0.0367\n",
      "Epoch 20/50\n",
      " - 5s - loss: 0.0339 - val_loss: 0.0357\n",
      "Epoch 21/50\n",
      " - 5s - loss: 0.0325 - val_loss: 0.0345\n",
      "Epoch 22/50\n",
      " - 5s - loss: 0.0307 - val_loss: 0.0329\n",
      "Epoch 23/50\n",
      " - 5s - loss: 0.0281 - val_loss: 0.0309\n",
      "Epoch 24/50\n",
      " - 5s - loss: 0.0241 - val_loss: 0.0276\n",
      "Epoch 25/50\n",
      " - 5s - loss: 0.0195 - val_loss: 0.0264\n",
      "Epoch 26/50\n",
      " - 5s - loss: 0.0183 - val_loss: 0.0259\n",
      "Epoch 27/50\n",
      " - 5s - loss: 0.0177 - val_loss: 0.0256\n",
      "Epoch 28/50\n",
      " - 5s - loss: 0.0170 - val_loss: 0.0259\n",
      "Epoch 29/50\n",
      " - 5s - loss: 0.0165 - val_loss: 0.0255\n",
      "Epoch 30/50\n",
      " - 5s - loss: 0.0161 - val_loss: 0.0252\n",
      "Epoch 31/50\n",
      " - 5s - loss: 0.0159 - val_loss: 0.0251\n",
      "Epoch 32/50\n",
      " - 5s - loss: 0.0157 - val_loss: 0.0246\n",
      "Epoch 33/50\n",
      " - 5s - loss: 0.0156 - val_loss: 0.0244\n",
      "Epoch 34/50\n",
      " - 5s - loss: 0.0154 - val_loss: 0.0241\n",
      "Epoch 35/50\n",
      " - 5s - loss: 0.0153 - val_loss: 0.0239\n",
      "Epoch 36/50\n",
      " - 5s - loss: 0.0151 - val_loss: 0.0236\n",
      "Epoch 37/50\n",
      " - 5s - loss: 0.0150 - val_loss: 0.0233\n",
      "Epoch 38/50\n",
      " - 5s - loss: 0.0149 - val_loss: 0.0231\n",
      "Epoch 39/50\n",
      " - 5s - loss: 0.0148 - val_loss: 0.0230\n",
      "Epoch 40/50\n",
      " - 6s - loss: 0.0147 - val_loss: 0.0228\n",
      "Epoch 41/50\n",
      " - 6s - loss: 0.0146 - val_loss: 0.0226\n",
      "Epoch 42/50\n",
      " - 6s - loss: 0.0145 - val_loss: 0.0225\n",
      "Epoch 43/50\n",
      " - 5s - loss: 0.0144 - val_loss: 0.0224\n",
      "Epoch 44/50\n",
      " - 6s - loss: 0.0143 - val_loss: 0.0222\n",
      "Epoch 45/50\n",
      " - 6s - loss: 0.0143 - val_loss: 0.0221\n",
      "Epoch 46/50\n",
      " - 5s - loss: 0.0142 - val_loss: 0.0220\n",
      "Epoch 47/50\n",
      " - 6s - loss: 0.0141 - val_loss: 0.0219\n",
      "Epoch 48/50\n",
      " - 6s - loss: 0.0140 - val_loss: 0.0218\n",
      "Epoch 49/50\n",
      " - 6s - loss: 0.0140 - val_loss: 0.0217\n",
      "Epoch 50/50\n",
      " - 6s - loss: 0.0139 - val_loss: 0.0216\n",
      "Wall time: 7min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# autoencoder\n",
    "X2_all_large = get_X2_features(occurences[occurences['id'] >= 5])\n",
    "ids = X2_all_large['id']\n",
    "X2_all_encoded = encode(X2_all_large.drop(columns=['id']), encoding_dim=32)\n",
    "X2_all_encoded['id'] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26508/26508 [02:32<00:00, 174.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 42min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# X1\n",
    "X1_all = pd.concat([X1, X1_test], ignore_index=True)\n",
    "bool_columns = [col for col in X1_all.columns if X1_all[col].nunique() == 2]\n",
    "not_bool_columns = list(set(X1_all.columns) - set(bool_columns) - set(['id']))\n",
    "# X1 categorical\n",
    "X1_categorical_embedding = umap.UMAP(n_components=3, metric='dice').fit_transform(X1_all[bool_columns])\n",
    "X1_categorical_embedding_df = pd.DataFrame(X1_categorical_embedding, columns=[f'X1_all_embedding_{i+1}' for i in range(3)])\n",
    "X1_categorical_embedding_df['id'] = X1_all['id']\n",
    "# X1 other\n",
    "X1_other_embedding = umap.UMAP().fit_transform(X1_all[not_bool_columns])\n",
    "X1_other_embedding_df = pd.DataFrame(X1_other_embedding, columns=['X1_all_embedding_5', 'X1_all_embedding_6'])\n",
    "X1_other_embedding_df['id'] = X1_all['id']\n",
    "# X2\n",
    "X2_all_cp = get_X2_features(occurences[occurences['id'] >= 5])\n",
    "X2_all_embedding = umap.UMAP(n_components=32, metric='dice').fit_transform(X2_all_cp.drop(columns=['id']))\n",
    "X2_all_embedding_df = pd.DataFrame(X2_all_embedding, columns=[f'X2_all_embedding_{i+1}' for i in range(32)])\n",
    "X2_all_embedding_df['id'] = X2_all_cp['id']\n",
    "# unite\n",
    "embeddings = [X1_categorical_embedding_df, X1_other_embedding_df, X2_all_embedding_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/embeddings_5_n.pickle','wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "umap...\n",
      "Wall time: 54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = agg_and_merge(X1, X2, X3, embeddings)\n",
    "X_test = agg_and_merge(X1_test, X2_test, X3_test, embeddings)\n",
    "X_train, X_test = eng(X_train, X_test)\n",
    "X_train, X_test = add_emedding_features(X_train, X_test, random_state=42)\n",
    "X_train_norm, X_test_norm = normalize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 304 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_norm = pd.merge(X_train_norm, X2_all_encoded, how='left', on='id')\n",
    "X_test_norm = pd.merge(X_test_norm, X2_all_encoded, how='left', on='id')\n",
    "X_train = pd.merge(X_train, X2_all_encoded, how='left', on='id')\n",
    "X_test = pd.merge(X_test, X2_all_encoded, how='left', on='id')\n",
    "\n",
    "X_train_norm.fillna(-999, inplace=True)\n",
    "X_test_norm.fillna(-999, inplace=True)\n",
    "X_train.fillna(-999, inplace=True)\n",
    "X_test.fillna(-999, inplace=True)\n",
    "assert len(X_train) == 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_norm_ext = pd.merge(X_train_norm, X2_all_large, how='left', on='id')\n",
    "X_test_norm_ext = pd.merge(X_test_norm, X2_all_large, how='left', on='id')\n",
    "X_train_ext = pd.merge(X_train, X2_all_large, how='left', on='id')\n",
    "X_test_ext = pd.merge(X_test, X2_all_large, how='left', on='id')\n",
    "\n",
    "X_train_norm_ext.fillna(-999, inplace=True)\n",
    "X_test_norm_ext.fillna(-999, inplace=True)\n",
    "X_train_ext.fillna(-999, inplace=True)\n",
    "X_test_ext.fillna(-999, inplace=True)\n",
    "assert len(X_train_norm_ext) == 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_1: 0.549, [0.5437299582848549, 0.5449122490683976, 0.5139370242009539, 0.5618934130749227, 0.5805197523849397]\n",
      "target_2: 0.618, [0.6043129527351447, 0.6121239009437767, 0.6010181818181818, 0.6303199404761904, 0.6444586541862806]\n",
      "target_3: 0.607, [0.5875517179348804, 0.6117143371109912, 0.6034869025735294, 0.6073494960133407, 0.6261058134191176]\n",
      "target_4: 0.604, [0.6094420929947245, 0.6058032207384131, 0.6353583532936198, 0.5837193666836595, 0.5872379072627936]\n",
      "target_5: 0.582, [0.573900148294612, 0.54798508884112, 0.5854133333333333, 0.5884937061755513, 0.6120513727517619]\n",
      "0.5921135553838036\n",
      "Wall time: 52.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "et_multi = ExtraTreesClassifier(n_estimators=1000,\n",
    "                                max_depth=7,\n",
    "                                class_weight='balanced',\n",
    "                                random_state=42,\n",
    "                                n_jobs=4)\n",
    "CV_multilabel(et_multi, X_train_ext, Y.drop(columns=['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_1: 0.549, [0.5564431453786998, 0.5474290780141844, 0.5089161408809503, 0.5653109097586019, 0.5653279105236402]\n",
      "target_2: 0.624, [0.6122783158946462, 0.6157739775752199, 0.6111930735930736, 0.6349330357142857, 0.6435335092922642]\n",
      "target_3: 0.603, [0.5777370030581039, 0.6345673682316963, 0.5924359489889706, 0.5767254707363674, 0.6332074333639707]\n",
      "target_4: 0.601, [0.6002944423997055, 0.5990699697003703, 0.6282209690000069, 0.5955478718295395, 0.5838768271403644]\n",
      "target_5: 0.593, [0.5831837972900683, 0.585718755952219, 0.5932, 0.5922319844555208, 0.6093983837174499]\n",
      "0.5938622128995966\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf_multi = RandomForestClassifier(n_estimators=1000,\n",
    "                                  max_depth=10,\n",
    "                                  class_weight='balanced',\n",
    "                                  random_state=42,\n",
    "                                  n_jobs=4)\n",
    "CV_multilabel(rf_multi, X_train_ext, Y.drop(columns=['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "step = 500\n",
    "intervals = [(i, i + step) for i in range(0, 4000, step)]\n",
    "splits = [[item for j, item in enumerate(intervals) if i != j] for i in range(len(intervals))]\n",
    "out_dims = [[item for j, item in enumerate(intervals) if i == j] for i in range(len(intervals))]\n",
    "X_train_splits = [pd.concat([X_train_ext[lb:ub] for (lb, ub) in split]) for split in splits]\n",
    "y_train_splits = [pd.concat([Y[lb:ub] for (lb, ub) in split]) for split in splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [04:53, 152.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "metafeatures = pd.concat([X_train[['id']], X_test[['id']]], ignore_index=True)\n",
    "for model_name, model in tqdm(zip(['rf_multi', 'et_multi'], [rf_multi, et_multi])):\n",
    "    X_train_predicts = []\n",
    "    X_test_predicts = []\n",
    "    for X_train_split, y_train_split, out_dim in zip(X_train_splits, y_train_splits, out_dims):\n",
    "        model.fit(X_train_split, y_train_split.drop(columns=['id']))\n",
    "        X_train_predicts.append(model.predict_proba(X_train_ext[out_dim[0][0]:out_dim[0][1]]))\n",
    "        X_test_predicts.append(model.predict_proba(X_test_ext))\n",
    "    \n",
    "    # X train features\n",
    "    X_train_features = {target: [] for target in targets}\n",
    "    for chunk in X_train_predicts:\n",
    "        for target, probas in zip(targets, chunk):\n",
    "            X_train_features[target].append(probas[:, 1])\n",
    "    for target in targets:\n",
    "        X_train_features[target] = np.concatenate(X_train_features[target])\n",
    "    # X test features\n",
    "    X_test_features = {target: [] for target in targets}\n",
    "    for chunk in X_test_predicts:\n",
    "        for target, probas in zip(targets, chunk):\n",
    "            X_test_features[target].append(probas[:, 1])\n",
    "    for target in targets:\n",
    "        X_test_features[target] = sum(X_test_features[target]) / len(X_test_features[target])\n",
    "    # metafeatures\n",
    "    for target in targets:\n",
    "        metafeatures[f'meta_{model_name}_{target}'] = np.concatenate((X_train_features[target], X_test_features[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 169 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_norm = pd.merge(X_train_norm, metafeatures, on='id')\n",
    "X_test_norm = pd.merge(X_test_norm, metafeatures, on='id')\n",
    "X_train = pd.merge(X_train, metafeatures, on='id')\n",
    "X_test = pd.merge(X_test, metafeatures, on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-04T08:33:37.485178Z",
     "start_time": "2019-09-04T08:33:37.478886Z"
    }
   },
   "source": [
    "# Simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(X_train, Y, on='id')\n",
    "logreg_merged = pd.merge(X_train_norm, Y, on='id')\n",
    "assert len(merged) == len(logreg_merged)\n",
    "final_models = {}\n",
    "final_models_roc_auc = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logregs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc  avg:  0.597   ['0.581', '0.566', '0.598', '0.608', '0.634']\n",
      "roc_auc  avg:  0.624   ['0.618', '0.636', '0.643', '0.607', '0.615']\n",
      "roc_auc  avg:  0.619   ['0.597', '0.614', '0.641', '0.646', '0.595']\n",
      "roc_auc  avg:  0.613   ['0.640', '0.626', '0.618', '0.586', '0.593']\n",
      "roc_auc  avg:  0.618   ['0.635', '0.613', '0.638', '0.619', '0.583']\n",
      "0.6139272302796444\n",
      "Wall time: 9.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logreg = LogisticRegression(C=0.05, class_weight='balanced', random_state=42, n_jobs=4)\n",
    "roc_aucs = []\n",
    "for target in targets:\n",
    "    roc_aucs.append(CV_metrics(logreg, X_train_norm, Y[target].values))\n",
    "print(np.mean(roc_aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc  avg:  0.596   ['0.553', '0.589', '0.606', '0.595', '0.640']\n",
      "roc_auc  avg:  0.622   ['0.619', '0.638', '0.635', '0.607', '0.610']\n",
      "roc_auc  avg:  0.613   ['0.603', '0.614', '0.622', '0.613', '0.612']\n",
      "roc_auc  avg:  0.616   ['0.630', '0.617', '0.636', '0.591', '0.608']\n",
      "roc_auc  avg:  0.606   ['0.618', '0.621', '0.613', '0.621', '0.557']\n",
      "0.610700171713918\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier(n_estimators=1000,\n",
    "                            max_depth=6,\n",
    "                            class_weight='balanced',\n",
    "                            random_state=42,\n",
    "                            n_jobs=4)\n",
    "roc_aucs = []\n",
    "for target in targets:\n",
    "    roc_aucs.append(CV_metrics(rf, X_train, Y[target].values))\n",
    "print(np.mean(roc_aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc  avg:  0.576   ['0.520', '0.569', '0.586', '0.579', '0.628']\n",
      "roc_auc  avg:  0.622   ['0.619', '0.642', '0.634', '0.609', '0.605']\n",
      "roc_auc  avg:  0.615   ['0.601', '0.611', '0.623', '0.625', '0.613']\n",
      "roc_auc  avg:  0.616   ['0.639', '0.617', '0.626', '0.598', '0.599']\n",
      "roc_auc  avg:  0.593   ['0.605', '0.604', '0.597', '0.612', '0.548']\n",
      "0.6043487829418638\n",
      "Wall time: 49.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "et = ExtraTreesClassifier(n_estimators=1000,\n",
    "                          max_depth=9,\n",
    "                          class_weight='balanced',\n",
    "                          random_state=42,\n",
    "                          n_jobs=4)\n",
    "roc_aucs = []\n",
    "for target in targets:\n",
    "    roc_aucs.append(CV_metrics(et, X_train, Y[target].values))\n",
    "print(np.mean(roc_aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc  avg:  0.616   ['0.593', '0.603', '0.618', '0.617', '0.649']\n",
      "roc_auc  avg:  0.621   ['0.626', '0.635', '0.629', '0.608', '0.605']\n",
      "roc_auc  avg:  0.629   ['0.597', '0.620', '0.654', '0.643', '0.629']\n",
      "roc_auc  avg:  0.619   ['0.632', '0.633', '0.637', '0.591', '0.600']\n",
      "roc_auc  avg:  0.604   ['0.626', '0.601', '0.601', '0.634', '0.557']\n",
      "0.6175374189440908\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gbm = LGBMClassifier(num_leaves=3,\n",
    "                     learning_rate=0.05,\n",
    "                     reg_lambda=75.0,\n",
    "                     random_state=42,\n",
    "                     class_weight='balanced')\n",
    "roc_aucs = []\n",
    "for target in targets:\n",
    "    roc_aucs.append(CV_metrics(gbm, X_train, Y[target].values))\n",
    "print(np.mean(roc_aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_1\n",
      "target_2\n",
      "target_3\n",
      "target_4\n",
      "target_5\n",
      "Wall time: 35.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "probas = []\n",
    "X_train = merged.drop(columns=targets)\n",
    "X_train_logreg = logreg_merged.drop(columns=targets)\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    y_train = merged[target].values\n",
    "    # models\n",
    "    # lightgbm\n",
    "    y_proba = gbm.fit(X_train, y_train).predict_proba(X_test)[:, 1]\n",
    "    # random forest\n",
    "    y_proba += rf.fit(X_train, y_train).predict_proba(X_test)[:, 1]\n",
    "    # extra tree\n",
    "    y_proba += et.fit(X_train, y_train).predict_proba(X_test)[:, 1]\n",
    "    # logreg\n",
    "    y_proba += logreg.fit(X_train_norm, y_train).predict_proba(X_test_norm)[:, 1]\n",
    "    y_proba /= 4.0\n",
    "    final_proba = y_proba\n",
    "    probas.append(final_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame(probas).T\n",
    "baseline = pd.DataFrame(tmp.values, columns=['1', '2', '3', '4', '5'])\n",
    "baseline['id'] = X_test['id']\n",
    "baseline[['id', '1', '2', '3', '4', '5']].to_csv('baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "246.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
